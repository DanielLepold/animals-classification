2025-04-15 11:53:07,436 - INFO - Training model started .. 
2025-04-15 11:53:07,436 - INFO - Training configuration:
2025-04-15 11:53:07,436 - INFO -   num_epochs     = 80
2025-04-15 11:53:07,436 - INFO -   batch_size     = 32
2025-04-15 11:53:07,436 - INFO -   hidden_units   = 20
2025-04-15 11:53:07,436 - INFO -   learning_rate  = 0.0001
2025-04-15 11:53:07,724 - INFO - Number of classes = 10
2025-04-15 11:53:07,724 - INFO - Model type: ModelType.TINY_VGG
2025-04-15 11:53:07,724 - INFO - Model configuration: 
TinyVGG(
  (conv_block_1): Sequential(
    (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block_2): Sequential(
    (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=3380, out_features=10, bias=True)
  )
)
2025-04-15 11:53:48,466 - INFO - Epoch: 1 | train_loss: 2.1492 | train_acc: 0.2205 | test_loss: 2.0104 | test_acc: 0.2902
2025-04-15 11:54:29,066 - INFO - Epoch: 2 | train_loss: 1.9577 | train_acc: 0.3100 | test_loss: 1.8904 | test_acc: 0.3345
2025-04-15 11:55:10,882 - INFO - Epoch: 3 | train_loss: 1.8655 | train_acc: 0.3470 | test_loss: 1.8339 | test_acc: 0.3579
2025-04-15 11:55:51,805 - INFO - Epoch: 4 | train_loss: 1.8072 | train_acc: 0.3690 | test_loss: 1.7904 | test_acc: 0.3809
2025-04-15 11:56:32,727 - INFO - Epoch: 5 | train_loss: 1.7468 | train_acc: 0.3918 | test_loss: 1.7336 | test_acc: 0.4015
2025-04-15 11:57:14,936 - INFO - Epoch: 6 | train_loss: 1.6963 | train_acc: 0.4149 | test_loss: 1.6758 | test_acc: 0.4116
2025-04-15 11:57:55,971 - INFO - Epoch: 7 | train_loss: 1.6546 | train_acc: 0.4344 | test_loss: 1.6550 | test_acc: 0.4212
2025-04-15 11:58:37,029 - INFO - Epoch: 8 | train_loss: 1.6190 | train_acc: 0.4489 | test_loss: 1.6096 | test_acc: 0.4486
2025-04-15 11:59:18,084 - INFO - Epoch: 9 | train_loss: 1.5789 | train_acc: 0.4662 | test_loss: 1.5742 | test_acc: 0.4625
2025-04-15 12:00:00,192 - INFO - Epoch: 10 | train_loss: 1.5463 | train_acc: 0.4773 | test_loss: 1.5675 | test_acc: 0.4701
2025-04-15 12:00:41,296 - INFO - Epoch: 11 | train_loss: 1.5132 | train_acc: 0.4872 | test_loss: 1.5445 | test_acc: 0.4669
2025-04-15 12:01:38,788 - INFO - Training model started .. 
2025-04-15 12:01:38,789 - INFO - Training configuration:
2025-04-15 12:01:38,789 - INFO -   num_epochs     = 80
2025-04-15 12:01:38,789 - INFO -   batch_size     = 32
2025-04-15 12:01:38,789 - INFO -   hidden_units   = 40
2025-04-15 12:01:38,789 - INFO -   learning_rate  = 0.0001
2025-04-15 12:01:39,071 - INFO - Number of classes = 10
2025-04-15 12:01:39,071 - INFO - Model type: ModelType.TINY_VGG
2025-04-15 12:01:39,071 - INFO - Model configuration: 
TinyVGG(
  (conv_block_1): Sequential(
    (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block_2): Sequential(
    (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=6760, out_features=10, bias=True)
  )
)
2025-04-15 12:02:21,612 - INFO - Epoch: 1 | train_loss: 2.0432 | train_acc: 0.2716 | test_loss: 1.8397 | test_acc: 0.3584
2025-04-15 12:03:04,930 - INFO - Epoch: 2 | train_loss: 1.7980 | train_acc: 0.3756 | test_loss: 1.7552 | test_acc: 0.3879
2025-04-15 12:03:47,463 - INFO - Epoch: 3 | train_loss: 1.6962 | train_acc: 0.4140 | test_loss: 1.6845 | test_acc: 0.4113
2025-04-15 12:04:29,715 - INFO - Epoch: 4 | train_loss: 1.5807 | train_acc: 0.4603 | test_loss: 1.5737 | test_acc: 0.4546
2025-04-15 12:05:13,072 - INFO - Epoch: 5 | train_loss: 1.4877 | train_acc: 0.4953 | test_loss: 1.4995 | test_acc: 0.4924
2025-04-15 12:05:55,623 - INFO - Epoch: 6 | train_loss: 1.4264 | train_acc: 0.5237 | test_loss: 1.4481 | test_acc: 0.5030
2025-04-15 12:06:39,281 - INFO - Epoch: 7 | train_loss: 1.3675 | train_acc: 0.5408 | test_loss: 1.4227 | test_acc: 0.5188
2025-04-15 12:07:21,696 - INFO - Epoch: 8 | train_loss: 1.3250 | train_acc: 0.5558 | test_loss: 1.3748 | test_acc: 0.5372
2025-04-15 12:08:03,942 - INFO - Epoch: 9 | train_loss: 1.2807 | train_acc: 0.5718 | test_loss: 1.3520 | test_acc: 0.5470
2025-04-15 12:08:46,871 - INFO - Epoch: 10 | train_loss: 1.2445 | train_acc: 0.5811 | test_loss: 1.3357 | test_acc: 0.5621
2025-04-15 12:09:29,306 - INFO - Epoch: 11 | train_loss: 1.2061 | train_acc: 0.5982 | test_loss: 1.3506 | test_acc: 0.5482
2025-04-15 12:10:12,351 - INFO - Epoch: 12 | train_loss: 1.1716 | train_acc: 0.6062 | test_loss: 1.3358 | test_acc: 0.5609
2025-04-15 12:10:54,390 - INFO - Epoch: 13 | train_loss: 1.1392 | train_acc: 0.6205 | test_loss: 1.3013 | test_acc: 0.5704
2025-04-15 12:11:36,561 - INFO - Epoch: 14 | train_loss: 1.1059 | train_acc: 0.6315 | test_loss: 1.2814 | test_acc: 0.5746
2025-04-15 12:12:19,873 - INFO - Epoch: 15 | train_loss: 1.0776 | train_acc: 0.6396 | test_loss: 1.3194 | test_acc: 0.5575
2025-04-15 12:13:02,271 - INFO - Epoch: 16 | train_loss: 1.0432 | train_acc: 0.6536 | test_loss: 1.2666 | test_acc: 0.5862
2025-04-15 12:13:44,420 - INFO - Epoch: 17 | train_loss: 1.0172 | train_acc: 0.6596 | test_loss: 1.2601 | test_acc: 0.5814
2025-04-15 12:14:27,416 - INFO - Epoch: 18 | train_loss: 0.9829 | train_acc: 0.6735 | test_loss: 1.2593 | test_acc: 0.5805
2025-04-15 12:15:09,706 - INFO - Epoch: 19 | train_loss: 0.9619 | train_acc: 0.6815 | test_loss: 1.2808 | test_acc: 0.5836
2025-04-15 12:15:53,020 - INFO - Epoch: 20 | train_loss: 0.9372 | train_acc: 0.6903 | test_loss: 1.2458 | test_acc: 0.5943
2025-04-15 12:16:35,662 - INFO - Epoch: 21 | train_loss: 0.9060 | train_acc: 0.6957 | test_loss: 1.2553 | test_acc: 0.5928
2025-04-15 12:17:17,966 - INFO - Epoch: 22 | train_loss: 0.8766 | train_acc: 0.7087 | test_loss: 1.2647 | test_acc: 0.5932
2025-04-15 12:18:00,736 - INFO - Epoch: 23 | train_loss: 0.8585 | train_acc: 0.7152 | test_loss: 1.3132 | test_acc: 0.5824
2025-04-15 12:18:42,960 - INFO - Epoch: 24 | train_loss: 0.8364 | train_acc: 0.7196 | test_loss: 1.2866 | test_acc: 0.5894
2025-04-15 12:19:26,221 - INFO - Epoch: 25 | train_loss: 0.8099 | train_acc: 0.7317 | test_loss: 1.2987 | test_acc: 0.5911
2025-04-15 12:20:08,550 - INFO - Epoch: 26 | train_loss: 0.7860 | train_acc: 0.7387 | test_loss: 1.2624 | test_acc: 0.5981
2025-04-15 12:20:50,490 - INFO - Epoch: 27 | train_loss: 0.7657 | train_acc: 0.7483 | test_loss: 1.2596 | test_acc: 0.6040
2025-04-15 12:21:33,540 - INFO - Epoch: 28 | train_loss: 0.7437 | train_acc: 0.7534 | test_loss: 1.3144 | test_acc: 0.5903
2025-04-15 12:22:15,955 - INFO - Epoch: 29 | train_loss: 0.7196 | train_acc: 0.7583 | test_loss: 1.3025 | test_acc: 0.5994
2025-04-15 12:22:59,119 - INFO - Epoch: 30 | train_loss: 0.6977 | train_acc: 0.7679 | test_loss: 1.3182 | test_acc: 0.5913
2025-04-15 12:23:41,460 - INFO - Epoch: 31 | train_loss: 0.6761 | train_acc: 0.7752 | test_loss: 1.3758 | test_acc: 0.5908
2025-04-15 12:24:23,717 - INFO - Epoch: 32 | train_loss: 0.6597 | train_acc: 0.7803 | test_loss: 1.3195 | test_acc: 0.6023
2025-04-15 12:25:06,607 - INFO - Epoch: 33 | train_loss: 0.6332 | train_acc: 0.7881 | test_loss: 1.3824 | test_acc: 0.6081
2025-04-15 12:25:48,523 - INFO - Epoch: 34 | train_loss: 0.6191 | train_acc: 0.7929 | test_loss: 1.3402 | test_acc: 0.6042
2025-04-15 12:26:31,724 - INFO - Epoch: 35 | train_loss: 0.5905 | train_acc: 0.8057 | test_loss: 1.4096 | test_acc: 0.6038
2025-04-15 12:27:14,578 - INFO - Epoch: 36 | train_loss: 0.5784 | train_acc: 0.8077 | test_loss: 1.4372 | test_acc: 0.6059
2025-04-15 12:27:56,835 - INFO - Epoch: 37 | train_loss: 0.5577 | train_acc: 0.8176 | test_loss: 1.4550 | test_acc: 0.5901
2025-04-15 12:28:39,981 - INFO - Epoch: 38 | train_loss: 0.5365 | train_acc: 0.8241 | test_loss: 1.5217 | test_acc: 0.5827
2025-04-15 12:29:22,198 - INFO - Epoch: 39 | train_loss: 0.5259 | train_acc: 0.8246 | test_loss: 1.5376 | test_acc: 0.5795
2025-04-15 12:30:05,509 - INFO - Epoch: 40 | train_loss: 0.4998 | train_acc: 0.8373 | test_loss: 1.5296 | test_acc: 0.5987
2025-04-15 12:30:47,568 - INFO - Epoch: 41 | train_loss: 0.4862 | train_acc: 0.8402 | test_loss: 1.5233 | test_acc: 0.5984
2025-04-15 12:31:29,721 - INFO - Epoch: 42 | train_loss: 0.4701 | train_acc: 0.8468 | test_loss: 1.6442 | test_acc: 0.5731
2025-04-15 12:32:12,757 - INFO - Epoch: 43 | train_loss: 0.4486 | train_acc: 0.8531 | test_loss: 1.5929 | test_acc: 0.5980
2025-04-15 12:32:55,003 - INFO - Epoch: 44 | train_loss: 0.4281 | train_acc: 0.8601 | test_loss: 1.6369 | test_acc: 0.5860
2025-04-15 12:33:37,782 - INFO - Epoch: 45 | train_loss: 0.4154 | train_acc: 0.8631 | test_loss: 1.7080 | test_acc: 0.5802
2025-04-15 12:34:20,655 - INFO - Epoch: 46 | train_loss: 0.4013 | train_acc: 0.8658 | test_loss: 1.7082 | test_acc: 0.5821
2025-04-15 12:35:02,732 - INFO - Epoch: 47 | train_loss: 0.3792 | train_acc: 0.8759 | test_loss: 1.7350 | test_acc: 0.5943
2025-04-15 12:35:45,913 - INFO - Epoch: 48 | train_loss: 0.3695 | train_acc: 0.8791 | test_loss: 1.7689 | test_acc: 0.5873
2025-04-15 12:36:28,058 - INFO - Epoch: 49 | train_loss: 0.3507 | train_acc: 0.8878 | test_loss: 1.8485 | test_acc: 0.5813
2025-04-15 12:37:10,170 - INFO - Epoch: 50 | train_loss: 0.3347 | train_acc: 0.8929 | test_loss: 1.9072 | test_acc: 0.5750
2025-04-15 12:37:53,237 - INFO - Epoch: 51 | train_loss: 0.3212 | train_acc: 0.8976 | test_loss: 1.9236 | test_acc: 0.5833
2025-04-15 12:38:35,565 - INFO - Epoch: 52 | train_loss: 0.3056 | train_acc: 0.9016 | test_loss: 1.9865 | test_acc: 0.5740
2025-04-15 12:39:18,791 - INFO - Epoch: 53 | train_loss: 0.2900 | train_acc: 0.9105 | test_loss: 2.0653 | test_acc: 0.5793
2025-04-15 12:40:00,988 - INFO - Epoch: 54 | train_loss: 0.2783 | train_acc: 0.9127 | test_loss: 2.1004 | test_acc: 0.5853
2025-04-15 12:40:42,822 - INFO - Epoch: 55 | train_loss: 0.2614 | train_acc: 0.9182 | test_loss: 2.1326 | test_acc: 0.5697
2025-04-15 12:41:25,893 - INFO - Epoch: 56 | train_loss: 0.2499 | train_acc: 0.9208 | test_loss: 2.3062 | test_acc: 0.5626
2025-04-15 12:42:07,942 - INFO - Epoch: 57 | train_loss: 0.2430 | train_acc: 0.9214 | test_loss: 2.2285 | test_acc: 0.5759
2025-04-15 12:42:51,199 - INFO - Epoch: 58 | train_loss: 0.2192 | train_acc: 0.9319 | test_loss: 2.2958 | test_acc: 0.5739
2025-04-15 12:43:33,581 - INFO - Epoch: 59 | train_loss: 0.2122 | train_acc: 0.9321 | test_loss: 2.3572 | test_acc: 0.5627
2025-04-15 12:44:15,657 - INFO - Epoch: 60 | train_loss: 0.2018 | train_acc: 0.9370 | test_loss: 2.3802 | test_acc: 0.5800
2025-04-15 12:44:58,855 - INFO - Epoch: 61 | train_loss: 0.1901 | train_acc: 0.9411 | test_loss: 2.4886 | test_acc: 0.5652
2025-04-15 12:45:41,199 - INFO - Epoch: 62 | train_loss: 0.1758 | train_acc: 0.9477 | test_loss: 2.5765 | test_acc: 0.5707
2025-04-15 12:46:23,723 - INFO - Epoch: 63 | train_loss: 0.1707 | train_acc: 0.9492 | test_loss: 2.5770 | test_acc: 0.5759
2025-04-15 12:47:06,607 - INFO - Epoch: 64 | train_loss: 0.1533 | train_acc: 0.9545 | test_loss: 2.6810 | test_acc: 0.5676
2025-04-15 12:47:48,910 - INFO - Epoch: 65 | train_loss: 0.1553 | train_acc: 0.9521 | test_loss: 2.7148 | test_acc: 0.5677
2025-04-15 12:48:32,409 - INFO - Epoch: 66 | train_loss: 0.1461 | train_acc: 0.9553 | test_loss: 2.8607 | test_acc: 0.5491
2025-04-15 12:49:14,904 - INFO - Epoch: 67 | train_loss: 0.1326 | train_acc: 0.9612 | test_loss: 2.8670 | test_acc: 0.5612
2025-04-15 12:49:57,248 - INFO - Epoch: 68 | train_loss: 0.1346 | train_acc: 0.9590 | test_loss: 2.9174 | test_acc: 0.5701
2025-04-15 12:50:40,798 - INFO - Epoch: 69 | train_loss: 0.1121 | train_acc: 0.9685 | test_loss: 3.0287 | test_acc: 0.5697
2025-04-15 12:51:23,379 - INFO - Epoch: 70 | train_loss: 0.1175 | train_acc: 0.9641 | test_loss: 3.0748 | test_acc: 0.5650
2025-04-15 12:52:05,698 - INFO - Epoch: 71 | train_loss: 0.1052 | train_acc: 0.9698 | test_loss: 3.1245 | test_acc: 0.5686
2025-04-15 12:52:49,056 - INFO - Epoch: 72 | train_loss: 0.1029 | train_acc: 0.9702 | test_loss: 3.2152 | test_acc: 0.5689
2025-04-15 12:53:31,711 - INFO - Epoch: 73 | train_loss: 0.1020 | train_acc: 0.9713 | test_loss: 3.2753 | test_acc: 0.5671
2025-04-15 12:54:15,243 - INFO - Epoch: 74 | train_loss: 0.0951 | train_acc: 0.9720 | test_loss: 3.2986 | test_acc: 0.5675
2025-04-15 12:54:15,244 - INFO - Stopping early at epoch 74 because train_loss < 0.2
2025-04-15 12:54:15,244 - INFO - [INFO] Saving model to: models/TINY_VGG_model_2.pth
2025-04-15 12:54:15,249 - INFO - Training model finished .. 
